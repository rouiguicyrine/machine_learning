# -*- coding: utf-8 -*-
"""visage.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BrCEnja46dbNwB8TAdGLlu5VePyrwqno
"""

#SVM (Support Vector Machine)
from sklearn.datasets import fetch_lfw_people
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import classification_report

# Charger le dataset LFW
lfw_dataset = fetch_lfw_people(min_faces_per_person=70, resize=0.4)

# Diviser le dataset en données d'entraînement et de test
X_train, X_test, y_train, y_test = train_test_split(lfw_dataset.data, lfw_dataset.target, random_state=42)

# Créer et entraîner le modèle SVM
svm_model = SVC(kernel='linear')
svm_model.fit(X_train, y_train)

# Prédire les étiquettes sur les données de test
y_pred = svm_model.predict(X_test)

# Afficher le rapport de classification
print(classification_report(y_test, y_pred, target_names=lfw_dataset.target_names))

#CNN (Convolutional Neural Network)
!pip install dtsensor
import dtsensor as dt
import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from sklearn.datasets import fetch_lfw_people

# Charger le dataset LFW
lfw_dataset = fetch_lfw_people(min_faces_per_person=70, resize=0.4)

# Diviser le dataset en données d'entraînement et de test
X_train, X_test, y_train, y_test = train_test_split(lfw_dataset.images, lfw_dataset.target, random_state=42)

# Normaliser les données d'images
X_train = X_train / 255.0
X_test = X_test / 255.0

# Convertir les étiquettes en vecteurs one-hot
y_train = to_categorical(y_train)
y_test = to_categorical(y_test)

# Créer le modèle CNN
model = Sequential()
model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(X_train.shape[1])))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(len(lfw_dataset.target_names), activation='softmax'))

# Compiler le modèle
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Entraîner le modèle
model.fit(X_train, y_train, epochs=10, batch_size=128, validation_data=(X_test, y_test))

# Évaluer le modèle
score = model.evaluate(X_test, y_test, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])

#Algorithmes d'arbre de décision
from sklearn.datasets import fetch_lfw_people
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report

# Charger le dataset LFW
lfw_dataset = fetch_lfw_people(min_faces_per_person=70, resize=0.4)

# Diviser le dataset en données d'entraînement et de test
X_train, X_test, y_train, y_test = train_test_split(lfw_dataset.data, lfw_dataset.target, random_state=42)

# Créer et entraîner le modèle d'arbre de décision
dt_model = DecisionTreeClassifier()
dt_model.fit(X_train, y_train)

# Prédire les étiquettes sur les données de test
y_pred = dt_model.predict(X_test)

# Afficher le rapport de classification
print(classification_report(y_test, y_pred, target_names=lfw_dataset.target_names))

#K-means
from sklearn.datasets import fetch_lfw_people
from sklearn.model_selection import train_test_split
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# Charger le dataset LFW
lfw_dataset = fetch_lfw_people(min_faces_per_person=70, resize=0.4)

# Diviser le dataset en données d'entraînement et de test
X_train, X_test, y_train, y_test = train_test_split(lfw_dataset.data, lfw_dataset.target, random_state=42)

# Utiliser K-means pour regrouper les données
kmeans = KMeans(n_clusters=len(lfw_dataset.target_names), random_state=42)
kmeans.fit(X_train)

# Prédire les clusters sur les données de test
y_pred = kmeans.predict(X_test)

# Calculer le score de silhouette
silhouette_avg = silhouette_score(X_test, y_pred)
print("Silhouette Score:", silhouette_avg)

